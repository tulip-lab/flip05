{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (05): Learning Theory (II)\n",
    "**(Module 04: Gaussian Process)**\n",
    "\n",
    "---\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use, but NOT allowed to change or distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "====================================================================\n",
    "Probabilistic predictions with Gaussian process classification (GPC)\n",
    "====================================================================\n",
    "This example illustrates the predicted probability of GPC for an RBF kernel\n",
    "with different choices of the hyperparameters. The first figure shows the\n",
    "predicted probability of GPC with arbitrarily chosen hyperparameters and with\n",
    "the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\n",
    "While the hyperparameters chosen by optimizing LML have a considerable larger\n",
    "LML, they perform slightly worse according to the log-loss on test data. The\n",
    "figure shows that this is because they exhibit a steep change of the class\n",
    "probabilities at the class boundaries (which is good) but have predicted\n",
    "probabilities close to 0.5 far away from the class boundaries (which is bad)\n",
    "This undesirable effect is caused by the Laplace approximation used\n",
    "internally by GPC.\n",
    "The second figure shows the log-marginal-likelihood for different choices of\n",
    "the kernel's hyperparameters, highlighting the two choices of the\n",
    "hyperparameters used in the first figure by black dots.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "\n",
    "# Generate data\n",
    "train_size = 50\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.uniform(0, 5, 100)[:, np.newaxis]\n",
    "y = np.array(X[:, 0] > 2.5, dtype=int)\n",
    "\n",
    "# Specify Gaussian Processes with fixed and optimized hyperparameters\n",
    "gp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),\n",
    "                                   optimizer=None)\n",
    "gp_fix.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "gp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\n",
    "gp_opt.fit(X[:train_size], y[:train_size])\n",
    "\n",
    "print(\"Log Marginal Likelihood (initial): %.3f\"\n",
    "      % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))\n",
    "print(\"Log Marginal Likelihood (optimized): %.3f\"\n",
    "      % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))\n",
    "\n",
    "print(\"Accuracy: %.3f (initial) %.3f (optimized)\"\n",
    "      % (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),\n",
    "         accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))\n",
    "print(\"Log-loss: %.3f (initial) %.3f (optimized)\"\n",
    "      % (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\n",
    "         log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))\n",
    "\n",
    "\n",
    "# Plot posteriors\n",
    "plt.figure(0)\n",
    "plt.scatter(X[:train_size, 0], y[:train_size], c='k', label=\"Train data\",\n",
    "            edgecolors=(0, 0, 0))\n",
    "plt.scatter(X[train_size:, 0], y[train_size:], c='g', label=\"Test data\",\n",
    "            edgecolors=(0, 0, 0))\n",
    "X_ = np.linspace(0, 5, 100)\n",
    "plt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], 'r',\n",
    "         label=\"Initial kernel: %s\" % gp_fix.kernel_)\n",
    "plt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], 'b',\n",
    "         label=\"Optimized kernel: %s\" % gp_opt.kernel_)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Class 1 probability\")\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(-0.25, 1.5)\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# Plot LML landscape\n",
    "plt.figure(1)\n",
    "theta0 = np.logspace(0, 8, 30)\n",
    "theta1 = np.logspace(-1, 1, 29)\n",
    "Theta0, Theta1 = np.meshgrid(theta0, theta1)\n",
    "LML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))\n",
    "        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]\n",
    "LML = np.array(LML).T\n",
    "plt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],\n",
    "         'ko', zorder=10)\n",
    "plt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],\n",
    "         'ko', zorder=10)\n",
    "plt.pcolor(Theta0, Theta1, LML)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.ylabel(\"Length-scale\")\n",
    "plt.title(\"Log-marginal-likelihood\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
